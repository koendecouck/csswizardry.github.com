---
layout: post
title: "Talks.uw - Week 3 - Reflection on doing surveys"
date: 2014-04-21 00:11:46
categories: General
meta: "talks-uw-week-3-reflection-on-doing-surveys"
---

<span style="line-height: 1.5em;">The UX research results are in. I'm totally excited! As I </span>announced<span style="line-height: 1.5em;"> last week I'm going to share those results and quote you a couple of sections from my report for capstone class. </span>

But today is Easter, so I'd rather do something a little more lighthearted. Research with people has its own quirks to it: participants don't always behave the way you imagine. It's what makes user research fun to do. Those quirks show up in the data you get back though, and that's how it ought to be. If you're not getting any weird stuff, chances are you phrased your questions so rigidly that you lost most of your  inter-subject response variation.

Let me illustrate that with a few graphs from my own survey.

<strong>Q1: "I'm currently registered as a..."</strong>

<a href="/_post_images/2014/04/1.png"><img class="alignleft size-medium wp-image-4179" alt="1" src="/_post_images/2014/04/1-300x179.png" width="300" height="179" /></a> For this study I specifically targeted UW graduate students (masters and PhD's) and UW faculty (both staff and instructors). I was very careful in distributing the survey link. Extra careful. Yet somehow three undergraduates still find their way there and submitted a survey for me. The only way that I was able to notice that is because I allowed an option for undergraduates in the first identifier item. Listing anticipated undesirables helps you to <em>identify unwanted test takers and remove them from your sample.</em>

<strong>Q2: "Affiliated to the college/school of..."</strong>

<a href="/_post_images/2014/04/2.png"><img class="alignleft size-medium wp-image-4180" alt="2" src="/_post_images/2014/04/2-300x179.png" width="300" height="179" /></a>You want your sample to be representative of your target population. There is a huge literature on what that means and how far you should take that. In this case, since I'm designing a product for the whole university I want each department to have a proportional voice in the matter. As you can see from the graph, about half of my responses came from inside the college of Arts and Sciences. This isn't too strange as that particular college is HUGE compared to the others: as its name suggests it holds every field in the arts and sciences, everything from drama to nuclear physics. Still, their input could be over-represented as compared to the others. Some colleges like Business and Dentistry are absent from the sample, which biases the results as well. The best way to verify representability would be to put up a second bar chart with actual enrollment figures. Unfortunately, the University of Washington doesn't seem to have a habit of posting its enrollment stats online. I'll have to go pull a few sleeves and look for it.

<strong>Q9: "Rank the following features in order of importance"</strong>

<a href="/_post_images/2014/04/3.png"><img class="alignleft size-medium wp-image-4181" alt="3" src="/_post_images/2014/04/3-290x300.png" width="290" height="300" /></a>Ranking desired features is common practice in product evaluation. However there are a couple of problems with it. First, your audience might not know exactly what it wants or which option it actually prefers. Ranking also doesn't convey any information on the strength of the conviction behind it. Participants get influenced in their ranking by the other items in the list and - to top it all of-  they get less accurate the more items you ask them to order for you.

Oh, and lest I forget: the way the question is phrased also matters. Believe it or not, ranking something from 1-6 is still ambiguous to some people. And you will get to hear it, no matter how carefully you tried phrasing it. I always encourage people to send me feedback though, just so I realize those issues exist.

One traditional way of obtaining a ranked list of desired features is to have it produced by a <strong>focus group</strong>. I did that, but decided not to waste much resources on it (I only tested a few people for 20 minutes). The reason for that is that your focus group is very small compared to your population. There is a high risk of bias. The ranking that is produced is negotiated within the group and much of the conflicting opinions on the matter is lost in the process. Even so, focus groups have their use.

Going by the focus group alone the final result would be:
<ol>
	<li>Keywords</li>
	<li>Speaker Bio</li>
	<li>Organizer</li>
	<li>Talks series info</li>
	<li>Freed food</li>
	<li>Videostreaming availability</li>
</ol>
After I got home, I added that particular question to the <strong>survey</strong> before sending it out. The graph you see here displays the preferences of 122 people. The conclusion is the same, but you'll notice that the level of agreement differs significantly for the different options. The data is much richer, with a lot more personal opinions (3 people rated 'free food' as a higher concern than getting a speaker bio for example).

<a href="/_post_images/2014/04/3.png"> </a>
